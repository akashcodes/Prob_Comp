\documentclass[12pt]{article}
\usepackage[ansinew]{inputenc} % ASCII (Western CP)
\usepackage{graphicx}
\usepackage{color}
\usepackage[colorlinks]{hyperref}
\usepackage{geometry}
\usepackage{amsmath}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{Probability and Computing - HW3}
\author{Pang Liang\\ Student No. 201418013229033}

\begin{document}
\maketitle

\section{Problem1}
Consider the probability that every bin receives exactly one ball when $n$ balls are thrown randomly into $n$ bins.
\begin{itemize}
\item Give an upper bound on this probability using the Poisson approximation.
\item Determine the exact probability of this event.
\end{itemize}

Answers:
\begin{itemize}
\item Using the Poisson approximation
we assume that there are $n$ independent Poisson distribution, and this can be a upper bound approximation as:
\begin{equation}
    \begin{split}
    (X_1^{(n)}, X_2^{(n)}, \dots, X_n^{(n)}) &\le e\sqrt{n} (Y_1^{(n)}, Y_2^{(n)}, \dots, Y_n^{(n)})\\
    &= e\sqrt{n} \prod_{i=1}^n e^{-1}\\
    &= e^{1-n}\sqrt{n}
    \end{split}
\end{equation}

\item We throw balls one by one then,
\begin{equation}
    \begin{split}
    (X_1^{(n)}, X_2^{(n)}, ..., X_n^{(n)}) &= (1-\frac{0}{n})(1-\frac{1}{n})\dots(1-\frac{n-1}{n})\\
    &= \frac{n!}{n^n}
    \end{split}
\end{equation}
\end{itemize}

\section{Problem2}
The following problem models a simple distributed system wherein agents contend for resources but back off in the face of contention. Balls represent agents, and bins represent resources.\\
The system evolves over rounds. Every round, balls are thrown independently and uniformly at random into $n$ bins. Any ball that lands in a bin by itself is served and removed from consideration. The remaining balls are thrown again in the next round. We begin with n balls in the first round, and we will finish when every ball is served.
\begin{itemize}
\item If there are $b$ balls at the start of a round, what is the expected number of balls at the start of the next round?
\item Suppose that every round the number of balls served was exactly the expected number of balls to be served. Show that all the balls would be served in $O(ln ln n)$ rounds. (Hint: If $x_j$ is the expected number of balls left after j rounds, show and use that $x_{j+1} \le x^2_j/n$.)

\end{itemize}

Answers:
\begin{itemize}
\item First we consider the probability $P(X_i)$ of one box is one ball,
\begin{equation}
    P(X_i) = (1-\frac{1}{n})^{b-1}*\frac{1}{n}*b
\end{equation}
So the expectation number of one ball box is:
\begin{equation}
    E[X] = E[\sum X_i] = \sum E[X_i] = n(1-\frac{1}{n})^{b-1}*\frac{1}{n}*b = b(1-\frac{1}{n})^{b-1}
\end{equation}
Then the question of the remain ball at the start of the next round is $b-b(1-\frac{1}{n})^{b-1}$.

\item
From Q1 we have
\begin{equation}
    \begin{split}
    x_{j+1} &= x_j - x_j(1-\frac{1}{n})^{x_j-1}\\
    &\le x_j - x_j (1-\frac{x_j-1}{n})\\
    &= \frac{x_j*x_j}{n}-\frac{x_j}{n}\\
    &\le \frac{x_j^2}{n}
    \end{split}
\end{equation}
\end{itemize}

Then using this we have:
\begin{equation}
    \begin{split}
    x_1 &\le \frac{x_0^2}{n} \\
    x_2 \le \frac{(\frac{x_0^2}{n})^2}{n} &= \frac{x_0^4}{n^3}\\
    &\cdots \\
    x_j &\le \frac{x_0^{2^j}}{n^{2^j-1}}
    \end{split}
\end{equation}
Let $x_{j-1}=1$, then all balls will be served in j rounds.
\begin{equation}
    \begin{split}
    n^{2^{j-1}-1} &\le x_0^{2^{j-1}} \\
    2^{j-1} &\le \frac{ln n}{ln n - ln x_0} \\
    j-1 &\le ln ln n - ln(ln n - ln x_0) \\
    \end{split}
\end{equation}
Because $x_0=n$, $j-1 = ln ln n$. We get $j = O(ln ln n)$.

\section{Problem3}
Let $X^{(m)}_i$ be the number of balls in bin i when m balls are independently and uniformly thrown at random into n bins, and $Y^{(m)}_i , 1 \le i \le n$, are independent Poisson random variables each having expectation $m/n$. Assume that $f$ is a nonnegative function.
\begin{itemize}
\item Prove that if $E[f(X_1^{(m)}, \dots, X_n^{(m)})]$ is monotonically increasing in $m$, then $E[f(X_1^{(m)}, \dots, X_n^{(m)})] \le 2E[f(Y_1^{(m)}, \dots, Y_n^{(m)})]$. (Hint: Show that $E[f(Y_1^{(m)}, \dots, Y_n^{(m)})] \ge E[f(X_1^{(m)}, \dots, X_n^{(m)})] Pr(\sum Y_i^{(m)} \ge m)$ and $Pr(\sum Y_i^{(m)} \ge m) \ge 1/2$.
\item (Bonus score 5 points) If $E[f(X_1^{(m)}, \dots, X_n^{(m)})]$ is monotonically decreasing in m, then $E[f(X_1^{(m)}, \dots, X_n^{(m)})] \le 2E[f(Y_1^{(m)}, \dots, Y_n^{(m)})]$.
\end{itemize}

Answers:
\begin{itemize}
\item First we use Poisson Approximation,
\begin{equation}
    \begin{split}
    E[f(Y_1^{(m)}, \dots, Y_n^{(m)})] &= \sum_{k=0}^{\infty} E[f(Y_1^{(m)}, \dots, Y_n^{(m)}) | \sum_{i=1}^{n}Y_{i}^{(m)}=k] Pr(\sum Y_i^{(m)} = k)\\
    &= \sum_{k=0}^{\infty} E[f(X_1^{(k)}, \dots, X_n^{(k)})] Pr(\sum Y_i^{(m)} = k)\\
    &\ge \sum_{k=m}^{\infty} E[f(X_1^{(k)}, \dots, X_n^{(k)})] Pr(\sum Y_i^{(m)} = k)\\
    &\ge \sum_{k=m}^{\infty} E[f(X_1^{(m)}, \dots, X_n^{(m)})] Pr(\sum Y_i^{(m)} = k) \\
    &\; Because\ the\ increase\ of\ E[f(X_1^{(m)}, \dots, X_n^{(m)})]\\
    &= E[f(X_1^{(m)}, \dots, X_n^{(m)})] Pr(\sum Y_i^{(m)} \ge m)
    \end{split}
\end{equation}
Then we set $Z = \sum Y_i^{(m)}$ obey the poisson distribution with mean $m$. So we consider the relation between $Pr(Z=i)$ and $Pr(Z=i+m)$ for $i<m$.
\begin{equation}
    \begin{split}
    \frac{Pr(Z=i+m)}{Pr(Z=i)} &= \frac{m^(i+m) e^{-m}}{(i+m)!} * \frac{i!}{m^i e^{-m}}\\
    &= \frac{m^m}{(i+1)(i+2)\dots(i+m)} \\
    &\ge 1
    \end{split}
\end{equation}
According to $Pr(Z=i) \le Pr(Z=i+m)$, we have $Pr(Z<m) \le Pr(m \le Z < 2m)$.
Because $1=Pr(Z<m) + Pr(Z\ge m)\le Pr(m \le Z < 2m) + Pr(Z\ge m) \le 2Pr(Z\ge m)$, we get $Pr(Z\ge m) \ge \frac{1}{2}$.

So combine these condition, we get the result $E[f(X_1^{(m)}, \dots, X_n^{(m)})] \le 2E[f(Y_1^{(m)}, \dots, Y_n^{(m)})]$.

\item
\begin{equation}
    \begin{split}
    E[f(Y_1^{(m)}, \dots, Y_n^{(m)})] &= \sum_{k=0}^{\infty} E[f(Y_1^{(m)}, \dots, Y_n^{(m)}) | \sum_{i=1}^{n}Y_{i}^{(m)}=k] Pr(\sum Y_i^{(m)} = k)\\
    &\ge \sum_{k=m}^{\infty} E[f(Y_1^{(m)}, \dots, Y_n^{(m)}) | \sum_{i=1}^{n}Y_{i}^{(m)}=k] Pr(\sum Y_i^{(m)} = k)\\
    &= \sum_{k=m}^{\infty} E[f(X_1^{(k)}, \dots, X_n^{(k)})] Pr(\sum Y_i^{(m)} = k)\\
    &\ge \sum_{k=m}^{\infty} E[f(X_1^{(m)}, \dots, X_n^{(m)})] Pr(\sum Y_i^{(m)} = k) \\
    &\; Because\ the\ decrease\ of\ E[f(X_1^{(m)}, \dots, X_n^{(m)})]\\
    &= E[f(X_1^{(m)}, \dots, X_n^{(m)})] Pr(\sum Y_i^{(m)} \ge m)
    \end{split}
\end{equation}
The other steps is follow the question above. So $E[f(X_1^{(m)}, \dots, X_n^{(m)})] \le 2E[f(Y_1^{(m)}, \dots, Y_n^{(m)})]$.
\end{itemize}

\section{Problem4}
Do Bernoulli experiment for 20 trials, using a new 1-Yuan coin. Record the result in a
string $s_1s_2 \cdots s_i \cdots s_{20}$, where $s_i$ is 1 if the $i^{th}$ trial gets Head, and otherwise is 0.

1110110100 1100001011

\end{document}
